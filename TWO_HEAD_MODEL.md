# Two-Head Spatial Anchor Calibration Model

**Status**: âœ… Ready for GPU Evaluation
**Branch**: `nyu-distance-evals`
**Date**: 2026-02-21

---

## Overview

Complete implementation of the spatial anchor calibration architecture from `Technical.md`. This is a **two-head model** that combines perception-based spatial calibration with VLM reasoning.

### Architecture

```
INPUT: Construction/Indoor Photo
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HEAD 1: PERCEPTION      â”‚
â”‚  â”œâ”€ Anchor Detection     â”‚  YOLOv8
â”‚  â”œâ”€ Depth Estimation     â”‚  Depth Anything V2 Large
â”‚  â””â”€ Spatial Calibration  â”‚  Known-dimension math
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ (calibrated measurements)
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HEAD 2: REASONING       â”‚
â”‚  VLM + Injected          â”‚  Claude Sonnet 4
â”‚  Measurements            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
     OUTPUT: Distance in meters
```

---

## Components Implemented

### 1. Anchor Detection (`models/anchor_detection.py`)

**Purpose**: Detect known-dimension objects for spatial calibration

**Models Supported**:
- YOLOv8 (general object detection)
- GroundTruthAnchorDetector (manual annotations for validation)

**Known Anchors**:
```python
ANCHOR_DIMENSIONS = {
    "2x4_stud_face": 3.5,      # inches
    "2x4_stud_edge": 1.5,
    "2x6_joist_face": 5.5,
    "cmu_block_width": 15.625,
    "cmu_block_height": 7.625,
    "rebar_4": 0.500,
    "rebar_5": 0.625,
    "electrical_box_single": 2.0,
    "electrical_box_double": 4.0,
    "door_opening_width": 38.5,
    "brick_length": 7.625,
    "brick_height": 2.25,
}
```

**Output**: List of `Anchor` objects with:
- Bounding box
- Known real-world dimension (inches)
- Pixel width
- Detection confidence

### 2. Depth Estimation (`models/depth_estimator.py`)

**Purpose**: Estimate relative depth map for plane grouping

**Model**: Depth Anything V2 Large (best accuracy)
- HuggingFace: `depth-anything/Depth-Anything-V2-Large`
- Input: RGB image
- Output: Normalized depth map [0, 1]

**Features**:
- Depth visualization with colormap
- Per-pixel depth queries
- Bounding box depth aggregation (median, mean, min, max)

### 3. Spatial Calibration (`models/spatial_calibration.py`)

**Purpose**: Convert pixel distances to physical measurements

**Algorithm**:
1. Group anchors by depth plane (using depth map)
2. For each plane, compute scale factor: `pixels_per_inch = pixel_width / known_width`
3. Cross-validate using multiple anchors (median scale, confidence from std dev)
4. Measure distances: `distance_inches = distance_pixels / pixels_per_inch`

**Output**: `DepthPlane` objects with:
- Calibrated scale (pixels/inch)
- Confidence score
- Number of anchors used
- Scale variance

**Key Math**:
```python
# Single anchor calibration
scale = anchor_pixel_width / anchor_real_width_inches

# Multi-anchor cross-validation
scales = [a.pixel_width / a.known_width for a in anchors]
calibrated_scale = median(scales)
confidence = 1.0 - (std(scales) / mean(scales))

# Distance measurement
distance_inches = pixel_distance / calibrated_scale
distance_meters = distance_inches * 0.0254
```

### 4. Two-Head VLM Client (`models/vlm_clients/twohead_client.py`)

**Purpose**: Integrate calibration with VLM reasoning

**Flow**:
1. Detect anchors in image
2. Estimate depth map
3. Calibrate spatial scale
4. Measure distance between query points
5. Inject measurement into VLM prompt
6. Return calibrated distance

**Fallback**: If no anchors detected, falls back to standard VLM estimation

**VLM**: Claude Sonnet 4 (can be swapped)

---

## Integration with Benchmark

The two-head model is integrated into `benchmark_all_models.py`:

```python
MODELS = {
    ...
    "two-head-claude": {
        "category": "two-head",
        "client_class": TwoHeadVLMClient,
        "model_name": "claude-sonnet-4-20250514",
        "requires_api": True,
        "requires_gpu": True,  # For Depth Anything V2 Large
        "env_var": "ANTHROPIC_API_KEY"
    }
}
```

**Visualization**: Two-head results appear in **red** on comparison charts

---

## Running on GPU Server

### Prerequisites

1. **Hardware**:
   - CUDA-capable GPU (16GB+ VRAM recommended)
   - 32GB+ RAM

2. **Software**:
   ```bash
   Python 3.9+
   CUDA 11.8+ with cuDNN
   ```

3. **API Keys** (in `.env`):
   ```bash
   ANTHROPIC_API_KEY=sk-ant-...
   OPENAI_API_KEY=sk-...
   GEMINI_API_KEY=AI...
   ```

### Installation

```bash
# Install GPU dependencies
pip install -r requirements-gpu.txt

# Verify CUDA
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### Run Full Benchmark

**Option 1: Interactive Script**
```bash
./run_full_benchmark_gpu.sh
```

**Option 2: Direct Command**
```bash
# Test two-head model only
python eval/nyu_distance/benchmark_all_models.py \
  --nyu_data_dir data/nyu_depth_v2/extracted \
  --num_images 10 \
  --pairs_per_image 3 \
  --models two-head-claude \
  --device cuda \
  --out_dir outputs/two_head_eval

# Test all models (API + Open + Two-Head)
python eval/nyu_distance/benchmark_all_models.py \
  --nyu_data_dir data/nyu_depth_v2/extracted \
  --num_images 20 \
  --pairs_per_image 3 \
  --device cuda \
  --out_dir outputs/full_benchmark
```

### Custom Test Sizes

```bash
# Quick test (3 images = 9 queries)
NUM_IMAGES=3 ./run_full_benchmark_gpu.sh

# Medium test (10 images = 30 queries) - default
NUM_IMAGES=10 PAIRS_PER_IMAGE=3 ./run_full_benchmark_gpu.sh

# Large test (50 images = 150 queries)
NUM_IMAGES=50 PAIRS_PER_IMAGE=3 ./run_full_benchmark_gpu.sh
```

---

## Expected Performance

### Baseline VLMs (No Calibration)
- **GPT-4o**: ~0.69m MAE (current best baseline)
- **Claude Sonnet 4**: ~0.73m MAE
- **Gemini 2.5 Pro**: TBD (very slow)

### Two-Head Model (With Calibration)
**Hypothesis**: Significant improvement when anchors are available

- **Best case** (many anchors, good coverage): <0.3m MAE
- **Good case** (2-3 anchors, high confidence): 0.3-0.5m MAE
- **Fallback** (no anchors): Same as baseline VLM (~0.7m MAE)

**Success depends on**:
1. Anchor detection accuracy (YOLO on generic model - room for improvement)
2. Number of anchors visible (more = better cross-validation)
3. Depth map quality (Depth Anything V2 is strong)
4. Scene suitability (construction scenes have more known-dimension objects)

### Inference Speed (per query)

- **Anchor Detection** (YOLO): ~0.1s
- **Depth Estimation** (Depth Anything V2 Large): ~1-2s
- **Calibration** (pure math): <0.01s
- **VLM Query** (Claude Sonnet 4): ~5s
- **Total**: ~6-7s per query

Compare to:
- GPT-4o baseline: ~3s
- Claude baseline: ~5s
- Two-head overhead: +1-2s (mostly depth estimation)

---

## Limitations & Future Work

### Current Limitations

1. **Generic YOLO Model**: Using pretrained YOLO on COCO dataset
   - Not trained on construction objects
   - Will miss studs, rebar, CMU blocks, etc.
   - **Mitigation**: Falls back to VLM when no anchors detected

2. **NYU Dataset**: Indoor residential scenes, not construction sites
   - Fewer known-dimension objects (no studs/rebar)
   - May detect: door frames, furniture (but dimensions vary)
   - **Result**: Model will likely fallback to VLM frequently

3. **Monocular Depth**: Relative depth only, not absolute
   - Cross-plane measurements are approximate
   - **Mitigation**: Focus on single-plane measurements

### Future Improvements

1. **Fine-tune YOLO on Construction Data**:
   ```bash
   # Annotate Ironsite videos with anchor bounding boxes
   # Train YOLOv8 on construction-specific classes
   # Expected: 10-20x better anchor detection
   ```

2. **Construction-Specific Dataset**:
   - Test on actual construction photos with visible anchors
   - Expected improvement: 2-3x better MAE

3. **Multi-View Fusion**:
   - Use ReID to match objects across multiple photos
   - Merge measurements for higher accuracy

4. **BIM Integration**:
   - Compare measurements to CAD specifications
   - Automated compliance checking

---

## Files Created

```
models/
â”œâ”€â”€ anchor_detection.py          # YOLO anchor detection
â”œâ”€â”€ depth_estimator.py           # Depth Anything V2 wrapper
â”œâ”€â”€ spatial_calibration.py       # Calibration math
â””â”€â”€ vlm_clients/
    â””â”€â”€ twohead_client.py        # Two-head integration

eval/nyu_distance/
â””â”€â”€ benchmark_all_models.py      # Updated with two-head model

requirements-gpu.txt              # GPU dependencies
run_full_benchmark_gpu.sh         # Automated benchmark runner
TWO_HEAD_MODEL.md                 # This file
```

---

## Quick Start (Copy-Paste for GPU Server)

```bash
# 1. Clone repo and switch to branch
git clone <repo-url>
cd IronsiteHackathon
git checkout nyu-distance-evals

# 2. Install dependencies
pip install -r requirements-gpu.txt

# 3. Set API keys
echo "ANTHROPIC_API_KEY=sk-ant-..." >> .env
echo "OPENAI_API_KEY=sk-..." >> .env
echo "GEMINI_API_KEY=AI..." >> .env

# 4. Run benchmark (interactive)
./run_full_benchmark_gpu.sh

# 5. View results
cat outputs/nyu_benchmark_gpu/benchmark_results.json | jq
open outputs/nyu_benchmark_gpu/benchmark_comparison.png
```

---

## Evaluation Questions

After running the benchmark, we can answer:

1. **Does spatial calibration improve accuracy?**
   - Compare two-head MAE vs baseline VLM MAE

2. **How often does calibration succeed?**
   - Check `calibrated: true/false` in results
   - NYU dataset may have low anchor detection rate

3. **What's the accuracy when calibration works?**
   - Filter results where `calibrated: true`
   - Compare MAE for calibrated vs uncalibrated queries

4. **Is the overhead worth it?**
   - +1-2s per query for depth estimation
   - If MAE improves by >30%, likely worth it

---

## Expected Benchmark Output

```
============================================================
BENCHMARK SUMMARY
============================================================
              model category      mae  median_error  success_rate
             gpt-4o   closed 0.689187      0.274120         100.0
    claude-sonnet-4   closed 0.728773      0.659645         100.0
     gemini-2.5-pro   closed      NaN           NaN           0.0
    two-head-claude two-head 0.XXXXXm      0.XXXXXm         100.0
============================================================

Visualization saved to: outputs/nyu_benchmark_gpu/benchmark_comparison.png
```

**Red bar** = Two-head model performance

---

## Contact & Support

**Implementation**: Complete âœ…
**Testing**: Requires GPU server ğŸ–¥ï¸
**Expected Runtime**: ~5-10 minutes for 30 queries

Ready to run! ğŸš€
